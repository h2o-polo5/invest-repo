{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 88046,
          "databundleVersionId": 10229277,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Santa_gk1",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h2o-polo5/invest-repo/blob/main/Santa_gk1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "uUFaEqb3-cTI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "santa_2024_path = kagglehub.competition_download('santa-2024')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Do1_4qJP-cTK"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T15:30:35.560513Z",
          "iopub.execute_input": "2024-12-16T15:30:35.561022Z",
          "iopub.status.idle": "2024-12-16T15:30:36.844046Z",
          "shell.execute_reply.started": "2024-12-16T15:30:35.560969Z",
          "shell.execute_reply": "2024-12-16T15:30:36.842612Z"
        },
        "id": "h5p604ck-cTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'Minimizing perplexity, a task quite grand'\n",
        "    'A neural network’s mission, across the land'\n",
        "    'A model, trained on texts, both vast and deep'\n",
        "    'To rearrange the words, while others sleep'\n",
        "    'It sorts and sifts, with algorithms bright'\n",
        "    'To find the clearest path, the words made right'\n",
        "    'A digital reindeer, with circuits aglow'\n",
        "    'Reducing confusion, a wondrous show'\n",
        "    'No longer puzzled, the reader’s mind at ease'\n",
        "    'As the model’s magic, effortlessly, please'\n",
        "    'A gift of clarity, a present divine'\n",
        "    'A neural network’s work, a clever design'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T15:46:01.557429Z",
          "iopub.execute_input": "2024-12-16T15:46:01.557999Z",
          "iopub.status.idle": "2024-12-16T15:46:01.565596Z",
          "shell.execute_reply.started": "2024-12-16T15:46:01.557949Z",
          "shell.execute_reply": "2024-12-16T15:46:01.56432Z"
        },
        "id": "VggWygL8-cTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'Minimizing perplexity, a task quite grand',\n",
        "    'A neural network’s mission, across the land.',\n",
        "    'A model, trained on texts, both vast and deep',\n",
        "    'To rearrange the words, while others sleep.'\n",
        "    'It sorts and sifts, with algorithms bright',\n",
        "    'To find the clearest path, the words made right.'\n",
        "    'A digital reindeer, with circuits aglow',\n",
        "    'Reducing confusion, a wondrous show.'\n",
        "    'No longer puzzled, the reader’s mind at ease',\n",
        "    'As the model’s magic, effortlessly, please.'\n",
        "    'A gift of clarity, a present divine',\n",
        "    'A neural network’s work, a clever design.'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T15:53:16.830925Z",
          "iopub.execute_input": "2024-12-16T15:53:16.831351Z",
          "iopub.status.idle": "2024-12-16T15:53:16.840024Z",
          "shell.execute_reply.started": "2024-12-16T15:53:16.831315Z",
          "shell.execute_reply": "2024-12-16T15:53:16.838755Z"
        },
        "id": "dqZEPwT--cTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'Minimizing perplexity, a task quite grand'\n",
        "    'A neural network’s mission, across the land'\n",
        "    'A model, trained on texts, both vast and deep'\n",
        "    'To rearrange the words, while others sleep'\n",
        "    'It sorts and sifts, with algorithms bright'\n",
        "    'To find the clearest path, the words made right'\n",
        "    'A digital reindeer, with circuits aglow'\n",
        "    'Reducing confusion, a wondrous show'\n",
        "    'No longer puzzled, the reader’s mind at ease'\n",
        "    'As the model’s magic, effortlessly, please'\n",
        "    'A gift of clarity, a present divine'\n",
        "    'A neural network’s work, a clever design'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences)\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "print(padded)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T16:00:57.433311Z",
          "iopub.execute_input": "2024-12-16T16:00:57.433797Z",
          "iopub.status.idle": "2024-12-16T16:00:57.446007Z",
          "shell.execute_reply.started": "2024-12-16T16:00:57.433758Z",
          "shell.execute_reply": "2024-12-16T16:00:57.443812Z"
        },
        "id": "i54ohoM3-cTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings\n",
        "\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def predict_order(sentences):\n",
        "    n = len(sentences)\n",
        "    scores = np.zeros((n, n))\n",
        "\n",
        "    # Calculate scores for all pairs of sentences\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i != j:\n",
        "                # Encode sentences\n",
        "                inputs = tokenizer(sentences[i], sentences[j], return_tensors='pt')\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                scores[i][j] = logits[0][1].item()  # Get score for 'is next'\n",
        "\n",
        "    # Order sentences based on scores\n",
        "    order = list(range(n))\n",
        "    order.sort(key=lambda x: sum(scores[x]), reverse=True)\n",
        "\n",
        "    return [sentences[i] for i in order]\n",
        "\n",
        "# Example scrambled sentences\n",
        "scrambled_sentences = [\n",
        "    \"Minimizing perplexity, a task quite grand,\",\n",
        "    \"A neural network’s mission, across the land.\",\n",
        "    \"A model, trained on texts, both vast and deep,\",\n",
        "    \"To rearrange the words, while others sleep.\",\n",
        "    \"It sorts and sifts, with algorithms bright,\",\n",
        "    \"To find the clearest path, the words made right.\",\n",
        "    \"A digital reindeer, with circuits aglow,\",\n",
        "    \"Reducing confusion, a wondrous show.\",\n",
        "    \"No longer puzzled, the reader’s mind at ease,\",\n",
        "    \"As the model’s magic, effortlessly, please.\",\n",
        "    \"A gift of clarity, a present divine,\",\n",
        "    \"A neural network’s work, a clever design.\"\n",
        "]\n",
        "\n",
        "# Predict and print ordered sentences\n",
        "ordered_sentences = predict_order(scrambled_sentences)\n",
        "for sentence in ordered_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-17T06:51:11.01455Z",
          "iopub.execute_input": "2024-12-17T06:51:11.015708Z",
          "iopub.status.idle": "2024-12-17T06:51:22.213183Z",
          "shell.execute_reply.started": "2024-12-17T06:51:11.01565Z",
          "shell.execute_reply": "2024-12-17T06:51:22.212084Z"
        },
        "id": "m2W1YUa9-cTN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from math import exp\n",
        "from collections import Counter\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Set environment variables for optimal performance\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "class PerplexityCalculator:\n",
        "    \"\"\"Calculates perplexity of text using a pre-trained language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, load_in_8bit: bool = False):\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model with or without 8-bit quantization\n",
        "        if load_in_8bit:\n",
        "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path, quantization_config=quantization_config).to(DEVICE)\n",
        "        else:\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(model_path).to(DEVICE)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 4) -> Union[float, List[float]]:\n",
        "        \"\"\"Calculates the perplexity of given texts.\"\"\"\n",
        "        single_input = isinstance(input_texts, str)\n",
        "        input_texts = [input_texts] if single_input else input_texts\n",
        "\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(input_texts), batch_size):\n",
        "                batch_texts = input_texts[i:i + batch_size]\n",
        "                for text in batch_texts:\n",
        "                    # Add special tokens for boundary conditions\n",
        "                    text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
        "                    model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False)\n",
        "\n",
        "                    # Move inputs to the appropriate device\n",
        "                    model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
        "\n",
        "                    # Get model output and calculate loss\n",
        "                    output = self.model(**model_inputs)\n",
        "                    logits = output.logits\n",
        "\n",
        "                    # Shift logits and labels for calculating loss\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "                    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "                    # Calculate average loss\n",
        "                    sequence_loss = loss.sum() / len(loss)\n",
        "                    loss_list.append(sequence_loss.cpu().item())\n",
        "\n",
        "        # Calculate perplexities from losses\n",
        "        perplexities = [exp(i) for i in loss_list]\n",
        "        return perplexities[0] if single_input else perplexities\n",
        "\n",
        "    def clear_gpu_memory(self) -> None:\n",
        "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "\n",
        "        del self.model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str,\n",
        "          model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
        "          load_in_8bit: bool = False, clear_mem: bool = False) -> float:\n",
        "    \"\"\"Calculates the mean perplexity of submitted text permutations compared to an original text.\"\"\"\n",
        "\n",
        "    # Check that each submitted string is a permutation of the solution string\n",
        "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
        "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
        "    invalid_mask = sol_counts != sub_counts\n",
        "\n",
        "    if invalid_mask.any():\n",
        "        raise ParticipantVisibleError('At least one submitted string is not a valid permutation of the solution string.')\n",
        "\n",
        "    # Calculate perplexity for the submitted strings\n",
        "    sub_strings = [' '.join(s.split()) for s in submission['text'].tolist()]  # Normalize whitespace\n",
        "\n",
        "    scorer = PerplexityCalculator(model_path=model_path, load_in_8bit=load_in_8bit)\n",
        "\n",
        "    # Get perplexities for each submitted string\n",
        "    perplexities = scorer.get_perplexity(sub_strings)\n",
        "\n",
        "    if clear_mem:\n",
        "        try:\n",
        "            scorer.clear_gpu_memory()\n",
        "        except Exception as e:\n",
        "            print(f'GPU memory clearing failed: {e}')\n",
        "\n",
        "    return float(np.mean(perplexities))\n",
        "\n",
        "# Example usage with provided sentences (replace with actual DataFrame inputs)\n",
        "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "solution_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"Minimizing perplexity, a task quite grand. A neural network’s mission, across the land. A model, trained on texts, both vast and deep. To rearrange the words, while others sleep. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right. A digital reindeer, with circuits aglow. Reducing confusion, a wondrous show. No longer puzzled, the reader’s mind at ease. As the model’s magic, effortlessly please. A gift of clarity, a present divine. A neural network’s work, a clever design.\"]\n",
        "})\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"A neural network’s mission, across the land. Minimizing perplexity, a task quite grand. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right.\"]\n",
        "})\n",
        "\n",
        "# Calculate score (perplexity) for submission against solution\n",
        "perplexity_score = score(solution_df, submission_df, 'id', clear_mem=True)\n",
        "print(f'Mean Perplexity Score: {perplexity_score}')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-17T06:51:54.027939Z",
          "iopub.execute_input": "2024-12-17T06:51:54.028325Z",
          "iopub.status.idle": "2024-12-17T06:51:54.543841Z",
          "shell.execute_reply.started": "2024-12-17T06:51:54.02829Z",
          "shell.execute_reply": "2024-12-17T06:51:54.542207Z"
        },
        "id": "J4QB7tXL-cTN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from math import exp\n",
        "from collections import Counter\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Set environment variables for optimal performance\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "class PerplexityCalculator:\n",
        "    \"\"\"Calculates perplexity of text using a pre-trained language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, load_in_8bit: bool = False):\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model with or without 8-bit quantization\n",
        "        if load_in_8bit:\n",
        "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path, quantization_config=quantization_config).to(DEVICE)\n",
        "        else:\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(model_path).to(DEVICE)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 4) -> Union[float, List[float]]:\n",
        "        \"\"\"Calculates the perplexity of given texts.\"\"\"\n",
        "        single_input = isinstance(input_texts, str)\n",
        "        input_texts = [input_texts] if single_input else input_texts\n",
        "\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(input_texts), batch_size):\n",
        "                batch_texts = input_texts[i:i + batch_size]\n",
        "                for text in batch_texts:\n",
        "                    # Add special tokens for boundary conditions\n",
        "                    text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
        "                    model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False)\n",
        "\n",
        "                    # Move inputs to the appropriate device\n",
        "                    model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
        "\n",
        "                    # Get model output and calculate loss\n",
        "                    output = self.model(**model_inputs)\n",
        "                    logits = output.logits\n",
        "\n",
        "                    # Shift logits and labels for calculating loss\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "                    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "                    # Calculate average loss\n",
        "                    sequence_loss = loss.sum() / len(loss)\n",
        "                    loss_list.append(sequence_loss.cpu().item())\n",
        "\n",
        "        # Calculate perplexities from losses\n",
        "        perplexities = [exp(i) for i in loss_list]\n",
        "        return perplexities[0] if single_input else perplexities\n",
        "\n",
        "    def clear_gpu_memory(self) -> None:\n",
        "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "\n",
        "        del self.model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str,\n",
        "          model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
        "          load_in_8bit: bool = False, clear_mem: bool = False) -> float:\n",
        "    \"\"\"Calculates the mean perplexity of submitted text permutations compared to an original text.\"\"\"\n",
        "\n",
        "    # Normalize text by lowercasing and stripping whitespace\n",
        "    solution['text'] = solution['text'].str.lower().str.strip()\n",
        "    submission['text'] = submission['text'].str.lower().str.strip()\n",
        "\n",
        "    # Check that each submitted string is a permutation of the solution string\n",
        "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
        "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
        "\n",
        "    invalid_mask = sol_counts != sub_counts\n",
        "\n",
        "    if invalid_mask.any():\n",
        "        print(\"Mismatch found:\")\n",
        "        for idx in invalid_mask[invalid_mask].index:\n",
        "            print(f\"Solution: {solution.loc[idx, 'text']}\")\n",
        "            print(f\"Submission: {submission.loc[idx, 'text']}\")\n",
        "            print(f\"Solution Count: {sol_counts[idx]}\")\n",
        "            print(f\"Submission Count: {sub_counts[idx]}\")\n",
        "\n",
        "        raise ParticipantVisibleError('At least one submitted string is not a valid permutation of the solution string.')\n",
        "\n",
        "    # Calculate perplexity for the submitted strings\n",
        "    sub_strings = [' '.join(s.split()) for s in submission['text'].tolist()]  # Normalize whitespace\n",
        "\n",
        "    scorer = PerplexityCalculator(model_path=model_path, load_in_8bit=load_in_8bit)\n",
        "\n",
        "    # Get perplexities for each submitted string\n",
        "    perplexities = scorer.get_perplexity(sub_strings)\n",
        "\n",
        "    if clear_mem:\n",
        "        try:\n",
        "            scorer.clear_gpu_memory()\n",
        "        except Exception as e:\n",
        "            print(f'GPU memory clearing failed: {e}')\n",
        "\n",
        "    return float(np.mean(perplexities))\n",
        "\n",
        "# Example usage with provided sentences (replace with actual DataFrame inputs)\n",
        "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "solution_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"Minimizing perplexity, a task quite grand. A neural network’s mission, across the land. A model, trained on texts, both vast and deep. To rearrange the words, while others sleep. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right. A digital reindeer, with circuits aglow. Reducing confusion, a wondrous show. No longer puzzled, the reader’s mind at ease. As the model’s magic, effortlessly please. A gift of clarity, a present divine. A neural network’s work, a clever design.\"]\n",
        "})\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"A neural network’s mission, across the land. Minimizing perplexity, a task quite grand. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right.\"]\n",
        "})\n",
        "\n",
        "# Calculate score (perplexity) for submission against solution\n",
        "try:\n",
        "    perplexity_score = score(solution_df, submission_df, 'id', clear_mem=True)\n",
        "    print(f'Mean Perplexity Score: {perplexity_score}')\n",
        "except ParticipantVisibleError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-17T07:00:38.031915Z",
          "iopub.execute_input": "2024-12-17T07:00:38.0323Z",
          "iopub.status.idle": "2024-12-17T07:00:38.057817Z",
          "shell.execute_reply.started": "2024-12-17T07:00:38.032265Z",
          "shell.execute_reply": "2024-12-17T07:00:38.056697Z"
        },
        "id": "LzAChBF6-cTO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from math import exp\n",
        "from collections import Counter\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Set environment variables for optimal performance\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "class PerplexityCalculator:\n",
        "    \"\"\"Calculates perplexity of text using a pre-trained language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, load_in_8bit: bool = False):\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model with or without 8-bit quantization\n",
        "        if load_in_8bit:\n",
        "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path, quantization_config=quantization_config).to(DEVICE)\n",
        "        else:\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(model_path).to(DEVICE)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 4) -> Union[float, List[float]]:\n",
        "        \"\"\"Calculates the perplexity of given texts.\"\"\"\n",
        "        single_input = isinstance(input_texts, str)\n",
        "        input_texts = [input_texts] if single_input else input_texts\n",
        "\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(input_texts), batch_size):\n",
        "                batch_texts = input_texts[i:i + batch_size]\n",
        "                for text in batch_texts:\n",
        "                    # Add special tokens for boundary conditions\n",
        "                    text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
        "                    model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False)\n",
        "\n",
        "                    # Move inputs to the appropriate device\n",
        "                    model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
        "\n",
        "                    # Get model output and calculate loss\n",
        "                    output = self.model(**model_inputs)\n",
        "                    logits = output.logits\n",
        "\n",
        "                    # Shift logits and labels for calculating loss\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "                    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "                    # Calculate average loss\n",
        "                    sequence_loss = loss.sum() / len(loss)\n",
        "                    loss_list.append(sequence_loss.cpu().item())\n",
        "\n",
        "        # Calculate perplexities from losses\n",
        "        perplexities = [exp(i) for i in loss_list]\n",
        "        return perplexities[0] if single_input else perplexities\n",
        "\n",
        "    def clear_gpu_memory(self) -> None:\n",
        "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "\n",
        "        del self.model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str,\n",
        "          model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
        "          load_in_8bit: bool = False, clear_mem: bool = False) -> float:\n",
        "    \"\"\"Calculates the mean perplexity of submitted text permutations compared to an original text.\"\"\"\n",
        "\n",
        "    # Normalize text by lowercasing and stripping whitespace\n",
        "    solution['text'] = solution['text'].str.lower().str.strip()\n",
        "    submission['text'] = submission['text'].str.lower().str.strip()\n",
        "\n",
        "    # Check that each submitted string is a permutation of the solution string\n",
        "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
        "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
        "\n",
        "    invalid_mask = sol_counts != sub_counts\n",
        "\n",
        "    if invalid_mask.any():\n",
        "        print(\"Mismatch found:\")\n",
        "        for idx in invalid_mask[invalid_mask].index:\n",
        "            print(f\"Solution: {solution.loc[idx, 'text']}\")\n",
        "            print(f\"Submission: {submission.loc[idx, 'text']}\")\n",
        "            print(f\"Solution Count: {sol_counts[idx]}\")\n",
        "            print(f\"Submission Count: {sub_counts[idx]}\")\n",
        "\n",
        "        raise ParticipantVisibleError('At least one submitted string is not a valid permutation of the solution string.')\n",
        "\n",
        "    # Calculate perplexity for the submitted strings\n",
        "    sub_strings = [' '.join(s.split()) for s in submission['text'].tolist()]  # Normalize whitespace\n",
        "\n",
        "    scorer = PerplexityCalculator(model_path=model_path, load_in_8bit=load_in_8bit)\n",
        "\n",
        "    # Get perplexities for each submitted string\n",
        "    perplexities = scorer.get_perplexity(sub_strings)\n",
        "\n",
        "    if clear_mem:\n",
        "        try:\n",
        "            scorer.clear_gpu_memory()\n",
        "        except Exception as e:\n",
        "            print(f'GPU memory clearing failed: {e}')\n",
        "\n",
        "    return float(np.mean(perplexities))\n",
        "\n",
        "# Example usage with provided sentences (replace with actual DataFrame inputs)\n",
        "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "solution_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"Minimizing perplexity, a task quite grand. A neural network’s mission, across the land. A model, trained on texts, both vast and deep. To rearrange the words, while others sleep. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right. A digital reindeer, with circuits aglow. Reducing confusion, a wondrous show. No longer puzzled, the reader’s mind at ease. As the model’s magic, effortlessly please. A gift of clarity, a present divine. A neural network’s work, a clever design.\"]\n",
        "})\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': [0],\n",
        "    'text': [\"A neural network’s mission, across the land. Minimizing perplexity, a task quite grand. It sorts and sifts, with algorithms bright. To find the clearest path, the words made right.\"]\n",
        "})\n",
        "\n",
        "# Calculate score (perplexity) for submission against solution\n",
        "try:\n",
        "    perplexity_score = score(solution_df, submission_df, 'id', clear_mem=True)\n",
        "    print(f'Mean Perplexity Score: {perplexity_score}')\n",
        "except ParticipantVisibleError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-17T07:02:50.124405Z",
          "iopub.execute_input": "2024-12-17T07:02:50.124833Z",
          "iopub.status.idle": "2024-12-17T07:02:50.151113Z",
          "shell.execute_reply.started": "2024-12-17T07:02:50.124796Z",
          "shell.execute_reply": "2024-12-17T07:02:50.149851Z"
        },
        "id": "3AE6VPCX-cTP"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}